<!DOCTYPE html>
<html ⚡ lang="en">
  <head>
  <script async custom-element="amp-youtube" src="https://cdn.ampproject.org/v0/amp-youtube-0.1.js"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Torch Tutorial for PlantVillage Challenge</title>
  <meta name="description" content="There is this interesting challenge called PlantVillage challenge hosted on a newly built platform, crowdai.In this challenge, you are required to identify t...">

  <link rel="canonical" href="https://chsasank.github.io//torch7-tutorial.html">
  <link rel="alternate" type="application/rss+xml" title="Sasank's Blog" href="https://chsasank.github.io//feed.xml">

  <script type="application/ld+json">
  
{
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": "https://chsasank.github.io//torch7-tutorial.html",
  "headline": "Torch Tutorial for PlantVillage Challenge",
  "datePublished": "2016-05-21T00:00:00+05:30",
  "dateModified": "2016-05-21T00:00:00+05:30",
  "description": "There is this interesting challenge called PlantVillage challenge hosted on a newly built platform, crowdai.In this challenge, you are required to identify t...",
  "author": {
    "@type": "Person",
    "name": "Sasank Chilamkurthy"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sasank's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chsasank.github.io/",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://chsasank.github.io/",
    "height": 60,
    "width": 60
  }
}

  </script>

  <style amp-custom>
  
  /* Import ET Book styles
   adapted from https://github.com/edwardtufte/et-book/blob/gh-pages/et-book.css */
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.eot");
  src: url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff") format("woff"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.ttf") format("truetype"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: normal; }
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.eot");
  src: url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff") format("woff"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.ttf") format("truetype"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: italic; }
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.eot");
  src: url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff") format("woff"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.ttf") format("truetype"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.svg#etbookromanosf") format("svg");
  font-weight: bold;
  font-style: normal; }
@font-face {
  font-family: "et-book-roman-old-style";
  src: url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.eot");
  src: url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff") format("woff"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.ttf") format("truetype"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: normal; }
/* Tufte CSS styles */
html {
  font-size: 15px; }

body {
  width: 87.5%;
  margin-left: auto;
  margin-right: auto;
  padding-left: 12.5%;
  font-family: et-book, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
  background-color: #fffff8;
  color: #111;
  max-width: 1400px;
  counter-reset: sidenote-counter; }

h1 {
  font-weight: 400;
  margin-top: 4rem;
  margin-bottom: 1.5rem;
  font-size: 3.2rem;
  line-height: 1; }

h2 {
  font-style: italic;
  font-weight: 400;
  margin-top: 2.1rem;
  margin-bottom: 0;
  font-size: 2.2rem;
  line-height: 1; }

h3 {
  font-style: italic;
  font-weight: 400;
  font-size: 1.7rem;
  margin-top: 2rem;
  margin-bottom: 0;
  line-height: 1; }

p.subtitle {
  font-style: italic;
  margin-top: 1rem;
  margin-bottom: 1rem;
  font-size: 1.8rem;
  display: block;
  line-height: 1; }

.numeral {
  font-family: et-book-roman-old-style; }

.danger {
  color: red; }

article {
  position: relative;
  padding: 5rem 0rem; }

section {
  padding-top: 1rem;
  padding-bottom: 1rem; }

p, ol, ul, .pagination a, .pagination em {
  font-size: 1.4rem; }

p {
  line-height: 2rem;
  margin-top: 1.4rem;
  margin-bottom: 1.4rem;
  padding-right: 0;
  vertical-align: baseline; }

/* Chapter Epigraphs */
div.epigraph {
  margin: 5em 0; }

div.epigraph > blockquote {
  margin-top: 3em;
  margin-bottom: 3em; }

div.epigraph > blockquote, div.epigraph > blockquote > p {
  font-style: italic; }

div.epigraph > blockquote > footer {
  font-style: normal; }

div.epigraph > blockquote > footer > cite {
  font-style: italic; }

/* end chapter epigraphs styles */
blockquote {
  font-size: 1.4rem; }

blockquote p {
  width: 50%; }

blockquote .footer {
  width: 50%;
  font-size: 1.1rem;
  text-align: right; }

ol, ul {
  width: 45%;
  -webkit-padding-start: 5%;
  -webkit-padding-end: 5%; }

li {
  padding: 0.5rem 0; }

figure {
  padding: 0;
  border: 0;
  font-size: 100%;
  font: inherit;
  vertical-align: baseline;
  max-width: 55%;
  -webkit-margin-start: 0;
  -webkit-margin-end: 0;
  margin: 0 0 3em 0; }

figcaption {
  float: right;
  clear: right;
  margin-right: -48%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.6;
  vertical-align: baseline;
  position: relative;
  max-width: 40%; }

figure.fullwidth figcaption {
  margin-right: 24%; }

/* Links: replicate underline that clears descenders */
a:link, a:visited {
  color: inherit; }

a:link {
  text-decoration: none;
  background: -webkit-linear-gradient(#fffff8, #fffff8), -webkit-linear-gradient(#fffff8, #fffff8), -webkit-linear-gradient(#333, #333);
  background: linear-gradient(#fffff8, #fffff8), linear-gradient(#fffff8, #fffff8), linear-gradient(#333, #333);
  -webkit-background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  -moz-background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  background-repeat: no-repeat, no-repeat, repeat-x;
  text-shadow: 0.03em 0 #fffff8, -0.03em 0 #fffff8, 0 0.03em #fffff8, 0 -0.03em #fffff8, 0.06em 0 #fffff8, -0.06em 0 #fffff8, 0.09em 0 #fffff8, -0.09em 0 #fffff8, 0.12em 0 #fffff8, -0.12em 0 #fffff8, 0.15em 0 #fffff8, -0.15em 0 #fffff8;
  background-position: 0% 93%, 100% 93%, 0% 93%; }

@media screen and (-webkit-min-device-pixel-ratio: 0) {
  a:link {
    background-position-y: 87%, 87%, 87%; } }
a:link::selection {
  text-shadow: 0.03em 0 #b4d5fe, -0.03em 0 #b4d5fe, 0 0.03em #b4d5fe, 0 -0.03em #b4d5fe, 0.06em 0 #b4d5fe, -0.06em 0 #b4d5fe, 0.09em 0 #b4d5fe, -0.09em 0 #b4d5fe, 0.12em 0 #b4d5fe, -0.12em 0 #b4d5fe, 0.15em 0 #b4d5fe, -0.15em 0 #b4d5fe;
  background: #b4d5fe; }

a:link::-moz-selection {
  text-shadow: 0.03em 0 #b4d5fe, -0.03em 0 #b4d5fe, 0 0.03em #b4d5fe, 0 -0.03em #b4d5fe, 0.06em 0 #b4d5fe, -0.06em 0 #b4d5fe, 0.09em 0 #b4d5fe, -0.09em 0 #b4d5fe, 0.12em 0 #b4d5fe, -0.12em 0 #b4d5fe, 0.15em 0 #b4d5fe, -0.15em 0 #b4d5fe;
  background: #b4d5fe; }

/* Sidenotes, margin notes, figures, captions */
img {
  max-width: 100%; }

.sidenote, .marginnote {
  float: right;
  clear: right;
  margin-right: -60%;
  width: 50%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.3;
  vertical-align: baseline;
  position: relative; }

.table-caption {
  float: right;
  clear: right;
  margin-right: -60%;
  width: 50%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.0rem;
  line-height: 1.6; }

.sidenote-number {
  counter-increment: sidenote-counter; }

.sidenote-number:after, .sidenote:before {
  content: counter(sidenote-counter) " ";
  font-family: et-book-roman-old-style;
  position: relative;
  vertical-align: baseline; }

.sidenote-number:after {
  content: counter(sidenote-counter);
  font-size: 1rem;
  top: -0.5rem;
  left: 0.1rem; }

.sidenote:before {
  content: counter(sidenote-counter) " ";
  top: -0.5rem; }

p, footer, table, div.table-wrapper-small, div.supertable-wrapper > p, div.booktabs-wrapper {
  width: 55%; }

div.fullwidth, table.fullwidth {
  width: 100%; }

div.table-wrapper {
  overflow-x: auto;
  font-family: "Trebuchet MS", "Gill Sans", "Gill Sans MT", sans-serif; }

@media screen and (max-width: 760px) {
  p, h1, h2, h3, footer {
    width: 90%; }

  pre.code {
    width: 87.5%; }

  ul {
    width: 85%; }

  figure {
    max-width: 90%; }

  figcaption, figure.fullwidth figcaption {
    margin-right: 0%;
    max-width: none; }

  blockquote p, blockquote .footer {
    width: 90%; } }
.sans {
  font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
  letter-spacing: .03em; }

.code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 1.125rem;
  line-height: 1.6; }

h1 .code, h2 .code, h3 .code {
  font-size: 0.80em; }

.marginnote .code, .sidenote .code {
  font-size: 1rem; }

pre.code {
  width: 52.5%;
  padding-left: 2.5%;
  overflow-x: auto; }

.fullwidth {
  max-width: 90%;
  clear: both; }

span.newthought {
  font-variant: small-caps;
  font-size: 1.2em; }

.margin-toggle {
  display: none; }

.sidenote-number {
  display: inline; }

.margin-toggle:not(.sidenote-number) {
  display: none; }

@media (max-width: 760px) {
  .margin-toggle:not(.sidenote-number) {
    display: none; }

  .sidenote, .marginnote {
    display: none; }

  .margin-toggle:checked + .sidenote,
  .margin-toggle:checked + .marginnote {
    display: block;
    float: left;
    left: 1rem;
    clear: both;
    width: 95%;
    margin: 1rem 2.5%;
    vertical-align: baseline;
    position: relative; }

  label {
    cursor: pointer; }

  pre.code {
    width: 90%;
    padding: 0; }

  .table-caption {
    display: block;
    float: right;
    clear: both;
    width: 98%;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    margin-left: 1%;
    margin-right: 1%;
    vertical-align: baseline;
    position: relative; }

  div.table-wrapper, table, table.booktabs {
    width: 85%; }

  div.table-wrapper {
    border-right: 1px solid #efefef; }

  img {
    width: 100%; } }
main {
  margin-top: 20px; }

amp-img {
  background-color: grey; }

article {
  padding: 2.5rem 0; }

header {
  margin-top: 20px; }

.post-meta {
  margin-top: 10px; }

pre {
  width: 52.5%;
  padding-left: 2.5%;
  overflow-x: auto; }

@media (max-width: 760px) {
  pre {
    width: 90%;
    padding: 0; } }
code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 1.125rem;
  line-height: 1.6; }

  </style>



  <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>

  <script async src="https://cdn.ampproject.org/v0.js"></script>
</head>

  <body>
    <header>
  <div class="page-links">
    <a class="page-link" href="/">Home</a>
    
      
      • <a class="page-link" href="/about/">About</a>
      
    
      
      • <a class="page-link" href="/contact/">Contact</a>
      
    
      
    
      
    
  </div>
</header>


    <article>

  <h2 itemprop="name"><a href="" itemprop="url">Torch Tutorial for PlantVillage Challenge</a></h2>

  <div class="post-meta">
    <time datetime="21 May 2016">21 May 2016</time>
  </div>

  <section>
    <p>There is this interesting challenge called <a href="https://www.crowdai.org/challenges/1">PlantVillage challenge</a> hosted on a newly built platform, <a href="https://www.crowdai.org">crowdai</a>.
In this challenge, you are required to identify the disease of a plant from an image of its leaf.</p>

<p>Dataset include both healthy and diseased leaves. Training dataset has 21917 images.
There are 38 classes of crop-disease pairs in the dataset:</p>

<figure>
  <amp-img width="600" height="500" layout="responsive" src="/assets/images/plantvillage/classes.jpg"></amp-img>
</figure>

<p>We’ll use popular deep learning platform <a href="http://torch.ch">torch</a> to solve this problem.
This will be a hands-on tutorial covering training of 
Alexnet
<span id="alexnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></span>.
Tutorial will be accompanied by a <a href="https://github.com/chsasank/plantvillage-challenge">repo containing complete working code</a>.
It will include VGGNet 
<span id="vggnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="http://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></span>
and ResNet 
<span id="resnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></span>.</p>

<p>This tutorial assumes familiarity with convolutional neural networks. If you are not, you can go through very readable <em>Neural Networks and Deep Learning</em> book by Michael Nielsen.
<a href="http://neuralnetworksanddeeplearning.com/chap6.html">Chapter 6</a> is the essential reading. 
Just read the <a href="http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks">first section</a> in this chapter if you are in a hurry. Go on, it’s a easy read. I’ll wait for you :).</p>

<p>Ok, Done? Let’s first start with preprocessing and augmentation of the images.</p>

<h2 id="preprocessing">Preprocessing</h2>
<p>Firstly, let’s download the dataset and extract it into a directory. 
We’ll divide the images into two directories, <code class="highlighter-rouge">train</code> and <code class="highlighter-rouge">val</code> for training and validation sets respectively.
I used a simple bash script to do this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">cd </span>directory/contaning/c_0c_1...etcdirectories
mkdir -p train val
<span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>0..37<span class="o">}</span>; <span class="k">do </span>mkdir val/c_<span class="nv">$i</span>; <span class="k">done
</span>mv c_<span class="k">*</span> train

<span class="nb">cd </span>train
find . -iname <span class="k">*</span>.jpg | shuf | head -n 2100| xargs -I<span class="o">{}</span> mv <span class="o">{}</span> ../val/<span class="o">{}</span>
</code></pre>
</div>

<p>This will move random 2100 images (about 10% of the dataset) in to <code class="highlighter-rouge">val</code> directory and rest into <code class="highlighter-rouge">train</code> directory.
Directory structure should now look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>.
├── train
│   ├── c_0
│   │   ├── img_name.JPG
│   │   ├── ...
│   │   └── img_name.JPG
│   ├── c_1
│   ├── ...
│   ├── c_36
│   └── c_37
│  
└── val
    ├── c_0
    ├── c_1
    ├── ...
    ├── c_36
    └── c_37
        ├── img_name.JPG
        ├── ...
        └── img_name.JPG
</code></pre>
</div>

<p>Before feeding images into neural networks we’ll normalize the images with mean and standard deviation of RGB channels computed from a random subset of ImageNet.</p>

<p>In the world of deep learning, dataset of 20,000 images is a relatively small dataset. 
We’ll therefore augment the data with randomly sized crops, color jittering and horizontal flips. 
Code to do these transformations is in <code class="highlighter-rouge">datasets/transforms.lua</code>. Most of it is borrowed from <a href="http://github.com/facebook/fb.resnet.torch">fb.resnet.torch</a> repo.</p>

<p>We will load the images in batches and do all these processing on the fly. This is done by writing a class named <code class="highlighter-rouge">DataGen</code> which does this. 
Essentially, code<span id="side1" class="margin-toggle sidenote-number"></span>
      <span class="sidenote">Understanding how iterators work in lua can be a little tricky. Read the following <a href="https://www.lua.org/pil/7.1.html"> documentation</a> for details.</span> can be summarized as :</p>

<p><code class="highlighter-rouge">datasets/plantvillage.lua</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'paths'
t = require 'datasets/transforms.lua'

function DataGen:__init(path)
    -- path is path of directory containing 'train' and 'val' folders
    self.rootPath = path
    self.trainImgPaths = self.findImages(paths.concat(self.rootPath, 'train'))
    self.valImgPaths = self.findImages(paths.concat(self.rootPath, 'val'))
    self.nbTrainExamples = #self.trainImgPaths
    self.nbValExamples = #self.valImgPaths 
end

-- Some utility functions
function DataGen.findImages(dir)
    -- Returns a table with all the image paths found in dir using 'find'
    ...
end

local function getClass(path)
    -- gets class from the name of the parent directory
    local className = paths.basename(paths.dirname(path))
    return tonumber(className:sub(3)) + 1
end

--- Iterator 
function DataGen:generator(pathsList, batchSize, preprocess) 
    -- pathsList is table with paths of images to be iterated over
    -- batchSize is number of images to be loaded in one iteration
    -- preprocess is function which will be applied to image after it's loaded

    -- Split all the paths into random batches
    local pathIndices = torch.randperm(#pathsList)
    local batches = pathIndices:split(batchSize)
    local i = 1
   
    return function ()
        if i &lt;= #batches then
            local currentBatch = batches[i]         

            local X = torch.Tensor(currentBatch:size(1), 3, 224, 224)
            local Y = torch.Tensor(currentBatch:size(1))

            for j = 1, currentBatch:size(1) do
                local currentPath = pathsList[currentBatch[j]]
                X[j] = preprocess(t.loadImage(currentPath))
                Y[j] = getClass(currentPath)
            end

            i = i + 1
            return X, Y
        end
   end
end

function DataGen:trainGenerator(batchSize)
    local trainPreprocess = t.Compose{
        t.RandomSizedCrop(224),
        t.ColorJitter({
            brightness = 0.4,
            contrast = 0.4,
            saturation = 0.4,
        }),
        t.Lighting(0.1, t.pca.eigval, t.pca.eigvec),
        t.ColorNormalize(t.meanstd),
        t.HorizontalFlip(0.5),}

   return self:generator(self.trainImgPaths, batchSize, trainPreprocess)
end


function DataGen:valGenerator(batchSize)
    local valPreprocess = t.Compose{
         t.Scale(256),
         t.ColorNormalize(t.meanstd),
         t.CenterCrop(224),}
   return self:generator(self.valImgPaths, batchSize, valPreprocess)
end

</code></pre>
</div>

<p>Complete code for this class with some error catching is at <code class="highlighter-rouge">datasets/plantvillage.lua</code>. 
We can now simply use a <code class="highlighter-rouge">DataGen</code> object to write a <code class="highlighter-rouge">for</code> loop to iterate over all the images:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>for input, target in dataGen:trainGenerator(batchSize) do
    -- code to train your model
end
</code></pre>
</div>

<p>Neat, isn’t it?</p>

<h2 id="models">Models</h2>
<p>Now that we’re done with loading and preprocessing the image files, let’s start writing model descriptions.
These are the actual powerhouses that will be trained to classify. 
It’s quite easy to code up a model.</p>

<p>Let’s code up alexnet as an example:</p>

<p><code class="highlighter-rouge">models/alexnet.lua</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'nn'

function createModel(opt)
    opt = opt or {}
    nbClasses = opt.nbClasses or 38
    nbChannels = opt.nbChannels or 3

    local features = nn.Sequential()
    
    features:add(nn.SpatialConvolution(nbChannels,64,11,11,4,4,2,2))    -- 224 -&gt; 55
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  55 -&gt;  27
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(64))
    
    features:add(nn.SpatialConvolution(64,192,5,5,1,1,2,2))       --  27 -&gt; 27
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  27 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(192))
    
    features:add(nn.SpatialConvolution(192,384,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(384))
    
    features:add(nn.SpatialConvolution(384,256,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(256))
    
    features:add(nn.SpatialConvolution(256,256,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  13 -&gt; 6
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(256))

    local classifier = nn.Sequential()
    classifier:add(nn.View(256*6*6))
    
    classifier:add(nn.Dropout(0.5))
    classifier:add(nn.Linear(256*6*6, 4096))
    classifier:add(nn.ReLU(true))
    
    classifier:add(nn.Dropout(0.5))
    classifier:add(nn.Linear(4096, 4096))
    classifier:add(nn.ReLU(true))
    
    classifier:add(nn.Linear(4096, nbClasses))

    model:add(features):add(classifier)

    return model
end
</code></pre>
</div>

<p>As you can see, model is just a stack of convolutional layers, max pooling and fully connected layers.</p>

<p>We will use <code class="highlighter-rouge">nn.CrossEntropyCriterion</code> as our criterion.</p>

<h2 id="training">Training</h2>

<p>In torch, <code class="highlighter-rouge">net:forward(input)</code> computes the <code class="highlighter-rouge">output</code> of neural network. 
This is the forward pass of the backpropagation algorithm while <code class="highlighter-rouge">net:backward(input,gradOutput)</code> is the backward pass of the backpropagation. 
Forward and backward passes for <code class="highlighter-rouge">criterion</code> are also very similar.</p>

<p>We’ll use <a href="https://github.com/torch/optim/blob/master/doc/index.md#optim.adam">adam</a> optimizer from <code class="highlighter-rouge">optim</code> package to make the updates to the network. A minimal training script will look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'nn'
require 'datasets/plantvillage.lua'
require 'models/alexnet.lua'

net = createModel()
criterion = nn.CrossEntropyCriterion()
dataGen = DataGen('path/to/folder/with/train-val-directories/')

-- adam initial learning rate and momentum parameters
optimState = { 
        learningRate = 0.01,
        beta1 = 0.9,
    }

-- Parameters for network that need to be optimized
params, gradParams = net:getParameters()

local function feval()
    return criterion.output, gradParams
end

function train()
    for input, target in dataGen:trainGenerator(batchSize) do
        -- Forward pass
        output = net:forward(input)
        criterion:forward(output, target)

        -- Backward pass
        net:zeroGradParameters()
        critGrad = criterion:backward(output, target)
        net:backward(input, critGrad)
        
        -- Make updates using adam
        optim.adam(feval, params, optimState)
    end
end

for i = 1, nbEpochs do
    train()
end
</code></pre>
</div>

<p>If you look at the code in the repo, you’ll find that I have divided training into <code class="highlighter-rouge">main.py</code> and <code class="highlighter-rouge">train.py</code> scripts.
In <code class="highlighter-rouge">main.lua</code>, we manage the configuration of the neural network and criterion.
In <code class="highlighter-rouge">train.lua</code>, I wrote a <code class="highlighter-rouge">Trainer</code> class with <code class="highlighter-rouge">Trainer:train()</code> and <code class="highlighter-rouge">Trainer:validate()</code> methods very similar to <code class="highlighter-rouge">train()</code> function above except with some logging.</p>

  </section>

    <footer class="site-footer">
   <section class="copyright">All content copyright <a href="mailto:sasankchilamkurthy@gmail.com">Sasank Chilamkurthy</a> &copy; 2016 &bull; All rights reserved.</section>
</footer>


  </div>
</article>

  </body>
</html>
