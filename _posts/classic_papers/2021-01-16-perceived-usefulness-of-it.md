---
layout: post
title: "Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology"
author: "Fred D. Davis"
category: classic_papers
description:
published: 1989-09-01
twitter_image: 
tag: "Social Sciences"
notes: 
---

## Abstract

Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both self-reported current usage (r=—.63, Study 1) and self-predicted future usage (r—=.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.

## Introduction

Information technology offers the potential for substantially improving white collar performance (Curley, 1984; Edelman, 1981; Sharda, et al., 1988). But performance gains are often obstructed by users’ unwillingness to accept and use available systems (Bowen, 1986; Young, 1984). Because of the persistence and importance of this problem, explaining user acceptance has been a long-standing issue in MIS research (Swanson, 1974; Lucas, 1975; Schultz and Slevin, 1975; Robey, 1979; Ginzberg, 1981; Swanson, 1987). Although numerous individual, organizational, and technological variables have been investigated (Benbasat and Dexter, 1986; Franz and Robey, 1986; Markus and Bjorn-Anderson, 1987; Robey and Farrow, 1982), research has been constrained by the shortage of high-quality measures for key determinants of user acceptance. Past research indicates that many measures do not correlate highly with system use (DeSanctis, 1983; Ginzberg, 1981; Schewe, 1976; Srinivasan, 1985), and the size of the usage correlation varies greatly from one study to the next depending on the particular measures used (Baroudi, et al., 1986; Barki and Huff, 1985; Robey, 1979; Swanson, 1982, 1987). The development of improved measures for key theoretical constructs is a research priority for the information systems field.

Aside from their theoretical value, better measures for predicting and explaining system use would have great practical value, both for vendors who would like to assess user demand for new design ideas, and for information systems managers within user organizations who would like to evaluate these vendor offerings.

Unvalidated measures are routinely used in practice today throughout the entire spectrum of design, selection, implementation and evaluation activities. For example: designers within vendor organizations such as IBM (Gould, et al., 1983), Xerox (Brewley, et al., 1983), and Digital Equipment Corporation (Good, et al., 1986) measure user perceptions to guide the development of new information technologies and products; industry publications often report user surveys (e.g., Greenberg, 1984; Rushinek and Rushinek, 1986); several methodologies for software selection call for subjective user inputs (e.g., Goslar, 1986; Klein and Beck, 1987); and contemporary design principles emphasize measuring user reactions throughout the entire design process (Anderson and Olson 1985; Gould and Lewis, 1985; Johansen and Baker, 1984; Mantei and Teorey, 1988; Norman, 1983; Shneiderman, 1987). Despite the widespread use of subjective measures in practice, little attention is paid to the quality of the measures used or how well they correlate with usage behavior. Given the low usage correlations often observed in research studies, those who base important business decisions on unvalidated measures may be getting misinformed about a system’s acceptability to users.


The purpose of this research is to pursue better measures for predicting and explaining use. The investigation focuses on two theoretical constructs, perceived usefulness and perceived ease of use, which are theorized to be fundamental determinants of system use. Definitions for these constructs are formulated and the theoretical rationale for their hypothesized influence on system use is reviewed. New, multi-item measurement scales for perceived usefulness and perceived ease of use are developed, pretested, and then validated in two separate empirical studies. Correlation and regression analyses examine the empirical relationship between the new measures and self-reported indicants of system use. The discussion concludes by drawing implications for future research.

## Perceived Usefulness and Perceived Ease of Use

What causes people to accept or reject information technology? Among the many variables that may influence system use, previous research suggests two determinants that are especially important. First, people tend to use or not use an application to the extent they believe it will help them perform their job better. We refer to this first variable as perceived usefulness. Second, even if potential users believe that a given application is useful, they may, at the same time, believe that the systems is too hard to use and that the performance benefits of usage are outweighed by the effort of using the application. That is, in addition to usefulness, usage is theorized to be influenced by perceived ease of use. 

Perceived usefulness is defined here as “the degree to which a person believes that using a particular system would enhance his or her job performance.” This follows from the definition of the word useful: “capable of being used advantageously.” Within an organizational context, people are generally reinforced for good performance by raises, promotions, bonuses, and other rewards (Pfeffer, 1982; Schein, 1980; Vroom, 1964). A system high in perceived usefulness, in turn, is one for which a user believes in the existence of a positive use-performance relationship.

Perceived ease of use, in contrast, refers to “the degree to which a person believes that using a particular system would be free of effort.” This follows from the definition of “ease”: “freedom from difficulty or great effort.” Effort is a finite resource that a person may allocate to the various activities for which he or she is responsible (Radner and Rothschild, 1975). All else being equal, we claim, an application perceived to be easier to use than another is more likely to be accepted by users.

## Theoretical Foundations


The theoretical importance of perceived usefulness and perceived ease of use as determinants of user behavior is indicated by several diverse lines of research. The impact of perceived usefulness on system utilization was suggested by the work of Schultz and Slevin (1975) and Robey (1979). Schultz and Slevin (1975) conducted an exploratory factor analysis of 67 questionnaire items, which yielded seven dimensions. Of these, the “performance” dimension, interpreted by the authors as the perceived “effect of the model on the manager’s job performance,” was most highly correlated with self-predicted use of a decision model (r= .61). Using the Schultz and Slevin questionnaire, Robey (1979) finds the performance dimension to be most correlated with two objective measures of system usage (r= .79 and .76). Building on Vertinsky, et al.’s (1975) expectancy model, Robey (1979) theorizes that: “A system that does not help people perform their jobs is not likely to be received favorably in spite of careful implementation efforts” (p. 537). Although the perceived use-performance contingency, as presented in Robey’s (1979) model, parallels our definition of perceived usefulness, the use of Schultz and Slevin’s (1975) performance factor to operationalize performance expectancies is problematic for several reasons: the instrument is empirically derived via exploratory factor analysis; a somewhat low ratio of sample size to items is used (2:1); four of thirteen items have loadings below .5, and several of the items clearly fall outside the definition of expected performance improvements (e.g., “My job will be more satisfying,” “Others will be more aware of what I am doing,” etc.).

An alternative expectancy-theoretic model, derived from Vroom (1964), was introduced and tested by DeSanctis (1983). The use-performance expectancy was not analyzed separately from performance-reward instrumentalities and reward valences. Instead, a matrix-oriented measurement procedure was used to produce an overall index of “motivational force” that combined these three constructs. “Force” had small but significant correlations with usage of a DSS within a business simulation experiment (correlations ranged from .04 to .26). The contrast between DeSanctis’s correlations and the ones observed by Robey underscore the importance of measurement in predicting and explaining use.

### Self-efficacy theory

The importance of perceived ease of use is supported by Bandura’s (1982) extensive research on self-efficacy, defined as “judgments of how well one can execute courses of action required to deal with prospective situations” (p. 122). Self-efficacy is similar to perceived ease of use as defined above. Self-efficacy beliefs are theorized to function as proximal determinants of behavior. Bandura’s theory distinguishes self-efficacy judgments from outcome judgments, the latter being concerned with the extent to which a behavior, once successfully executed, is believed to be linked to valued outcomes. Bandura’s “outcome judgment” variable is similar to perceived usefulness. Bandura argues that self-efficacy and outcome beliefs have differing antecedents and that, “In any given instance, behavior would be best predicted by considering both self-efficacy and outcome beliefs” (p. 140).

Hill, et al. (1987) find that both self-efficacy and outcome beliefs exert an influence on decisions to learn a computer language. The self efficacy paradigm does not offer a general measure applicable to our purposes since efficacy beliefs are theorized to be situationally-specific, with measures tailored to the domain under study (Bandura, 1982). Self efficacy research does, however, provide one of several theoretical perspectives suggesting that perceived ease of use and perceived usefulness function. as basic determinants of user behavior.

### Cost-benefit paradigm

The cost-benefit paradigm from behavioral decision theory (Beach and Mitchell, 1978; Johnson and Payne, 1985; Payne, 1982) is also relevant to perceived usefulness and ease of use. This research explains people’s choice among various decision-making strategies (such as linear compensatory, conjunctive, disjunctive and elmination-by-aspects) in terms of a cognitive tradeoff between the effort required to employ the strategy and the quality (accuracy) of the resulting decision. This approach has been effective for explaining why decision makers alter their choice strategies in response to changes in task complexity. Although the cost-benefit approach has mainly concerned itself with unaided decision making, recent work has begun to apply the same form of analysis to the effectiveness of information display formats (Jarvenpaa, 1989; Kleinmuntz and Schkade, 1988). 

Cost-benefit research has primarily used objective measures of accuracy and effort in research studies, downplaying the distinction between objective and subjective accuracy and effort. Increased emphasis on subjective constructs is warranted, however, since (1) a decision maker's choice of strategy is theorized to be based on subjective as opposed to objective accuracy and effort (Beach and Mitchell, 1978), and (2) other research suggests that subjective measures are often in disagreement with their objective counterparts (Abelson and Levi, 1985; Adelbratt and Montgomery, 1980; Wright, 1975). Introducing measures of the decision maker’s own perceived costs and benefits, independent of the decision actually made, has been suggested as a way of mitigating criticisms that the cost/ benefit framework is tautological (Abelson and Levi, 1985). The distinction made herein between perceived usefulness and perceived ease of use is similar to the distinction between subjective decision-making performance and effort.

### Adoption of innovations

Research on the adoption of innovations also suggests a prominent role for perceived ease of use. In their meta-analysis of the relationship between the characteristics of an innovation and its adoption, Tornatzky and Klein (1982) find that compatibility, relative advantage, and complexity have the most consistent significant relationships across a broad range of innovation types. Complexity, defined by Rogers and Shoemaker (1971) as “the degree to which an innovation is perceived as relatively difficult to understand and use” (p. 154), parallels perceived ease of use quite closely. As Tornatzky and Klein (1982) point out, however, compatibility and relative advantage have both been dealt with so broadly and inconsistently in the literature as to be difficult to interpret.

### Evaluation of information reports

Past research within MIS on the evaluation of information reports echoes the distinction between usefulness and ease of use made herein. Larcker and Lessig (1980) factor analyzed six items used to rate four information reports. Three items load on each of two distinct factors: (1) perceived importance, which Larcker and Lessig define as “the quality that causes a particular information set to acquire relevance to a decision maker,” and the extent to which the information elements are “a necessary input for task accomplishment,” and (2) perceived usableness, which is defined as the degree to which “the information format is unambiguous, clear or readable” (p. 123). These two dimensions are similar to perceived usefulness and perceived ease of use as defined above, respectively, although Larcker and Lessig refer to the two dimensions collectively as “perceived usefulness.” Reliabilities for the two dimensions fall in the range of .64-.77, short of the .80 minimal level recommended for basic research. Correlations with actual use of information reports were not addressed in their study.

### Channel disposition model

Swanson (1982, 1987) introduced and tested a model of “channel disposition” for explaining the choice and use of information reports. The concept of channel disposition is defined as having two components: attributed information quality
and attributed access quality. Potential users are hypothesized to select and use information reports based on an implicit psychological tradeoff between information quality and associated costs of access. Swanson (1987) performed an exploratory factor analysis in order to measure information quality and access quality. A five-factor solution was obtained, with one factor corresponding to information quality (Factor #3, “value”), and one to access quality (Factor #2, “accessibility”). Inspecting the items that load on these factors suggests a close correspondence to perceived usefulness and ease of use. Items such as “important,” “relevant,” “useful,” and “valuable” load strongly on the value dimension. Thus, value parallels perceived usefulness. The fact that relevance and usefulness load on the same factor agrees with information scientists, who emphasize the conceptual similarity between the usefulness and relevance notions (Saracevic, 1975). Several of Swanson’s “accessibility” items, such as “convenient,” ‘“‘controllable,” “easy,” and “unburdensome,” correspond to perceived ease of use as defined above. Although the study was more exploratory than confirmatory, with no attempts at construct validation, it does agree with the conceptual distinction between usefulness and ease of use. Self-reported information channel use correlated .20 with the value dimension and .13 with the accessibility dimension.

### Non-MIS studies

Outside the MIS domain, a marketing study by Hauser and Simmie (1981) concerning user perceptions of alternative communication technologies similarly derived two underlying dimensions: ease of use and effectiveness, the latter being similar to the perceived usefulness construct defined above. Both ease of use and effectiveness were influential in the formation of user preferences regarding a set of alternative communication technologies. The human-computer interaction (HCl) research community has heavily emphasized ease of use in design (Branscomb and Thomas, 1984; Card, et al., 1983; Gould and Lewis, 1985). For the most part, however, these studies have focused on objective measures of ease of use, such as task completion time and error rates. In many vendor organizations, usability testing has become a standard phase in the product development cycle, with large investments in test facilities and instrumentation. Although objective ease of use is clearly relevant to user performance given the system is used, subjective ease of use is more relevant to the users’ decision whether or not to use the system and may not agree with the objective measures (Carroll and Thomas, 1988).

### Convergence of findings

There is a striking convergence among the wide range of theoretical perspectives and research studies discussed above. Although Hill, et al. (1987) examined learning a computer language, Larcker and Lessig (1980) and Swanson (1982, 1987) dealt with evaluating information reports, and Hauser and Simmie (1981) studied communication technologies, all are supportive of the conceptual and empirical distinction between usefulness and ease of use. The accumulated body of knowledge regarding self-efficacy, contingent decision behavior and adoption of innovations provides theoretical support for perceived usefulness and ease of use as key determinants of behavior.

From multiple disciplinary vantage points, perceived usefulness and perceived ease of use are indicated as fundamental and distinct constructs that are influential in decisions to use information technology. Although certainly not the only variables of interest in explaining user behavior (for other variables, see Cheney, et al., 1986; Davis, et al., 1989; Swanson, 1988), they do appear likely to play a central role. Improved measures are needed to gain further insight into the nature of perceived usefulness and perceived ease of use, and their roles as determinants of computer use.

## Scale Development and Pretest

A step-by-step process was used to develop new multi-item scales having high reliability and validity. The conceptual definitions of perceived usefulness and perceived ease of use, stated above, were used to generate 14 candidate items for each construct from past literature. Pretest interviews were then conducted to assess the semantic content of the items. Those items that best fit the definitions of the constructs were retained, yielding 10 items for each construct. Next, a field study (Study 1) of 112 users concerning two different interactive computer systems was conducted in order to assess the reliability and construct validity of the resulting scales. The scales were further refined and streamlined to six items per construct. A lab study (Study 2) involving 40 participants and two graphics systems was then conducted. Data from the two studies were then used to assess the relationship between usefulness, ease of use, and self-reported usage.

Psychometricians emphasize that the validity of a measurement scale is built in from the outset. As Nunnally (1978) points out, “Rather than test the validity of measures after they have been constructed, one should ensure the validity by the plan and procedures for construction” (p. 258). Careful selection of the initial scale items helps to assure the scales will possess “content validity,” defined as “the degree to which the score or scale being used represents the concept about which generalizations are to be made” (Bohrnstedt, 1970, p. 91). In discussing content validity, psychometricians often appeal to the “domain sampling model,” (Bohrnstect, 1970; Nunnally, 1978) which assumes there is a domain of content corresponding to each variable one is interested in measuring. Candidate items representative of the domain of content should be selected. Researchers are advised to begin by formulating conceptual definitions of what is to be measured and preparing items to fit the construct definitions (Anastasi, 1986).

Following these recommendations, candidate items for perceived usefulness and perceived ease of use were generated based on their conceptual definitions, stated above, and then pretested in order to select those items that best fit the content domains. The Spearman-Brown Prophecy formula was used to choose the number of items to generate for each scale. This formula estimates the number of items needed to achieve a given reliability based on the number of items and reliability of comparable existing scales. Extrapolating from past studies, the formula suggests that 10 items would be needed for each perceptual variable to achieve reliability of at least .80 (Davis, 1986). Adding four additional items for each construct to allow for item elimination, it was decided to generate 14 items for each construct.

The initial item pools for perceived usefulness
and perceived ease of use are given in Tables q and 2, respectively. In preparing candidate items, 37 published research papers dealing with user reactions to interactive systems were reviewed in other to identify various facets of the constructs that should be measured (Davis, 1986). The items are worded in reference to “the electronic mail system,” which is one of the two test applications investigated in Study 1, reported below. The items within each pool tend to have a lot of overlap in their meaning, which is consistent with the fact that they are intended as measures of the same underlying construct. Though different individuals may attribute slightly different meaning to particular item statements, the goal of the multi-item approach is to reduce any extranneous effects of individual items, allowing idiosyncrasies to be cancelled out by other items in order to yield a more pure indicant of the conceptual variable.

Pretest interviews were performed to further enhance content validity by assessing the correspondence between candidate items and the definitions of the variables they are intended to measure. Items that don’t represent a construct’s content very well can be screened out by asking individuals to rank the degree to which each item matches the variable’s definition, and eliminating items receiving low rankings. In eliminating items, we want to make sure not to reduce the representativeness of the item pools. Our item pools may have excess coverage of some areas of meaning (or substrata; see Bohrnstedt, 1970) within the content domain and not enough of others. By asking individuals to rate the similarity of items to one another, we can perform a cluster analysis to determine the structure of the substrata, remove items where excess coverage is suggested, and add items where inadequate coverage is indicated.

Pretest participants consisted of a sample of 15 experienced computer users from the Sloan School of Management, MIT, including five secretaries, five graduate students and five members of the professional staff. In face-to-face interviews, participants were asked to perform two tasks, prioritization and categorization, which were done separately for usefulness and ease of use. For prioritization, they were first given a card containing the definition of the target construct and asked to read it. Next, they were given 13 index cards each having one of the items for that construct written on it. The 14th or “overall” item for each construct was omitted since its wording was almost identical to the label on the definition card (see Tables 1 and 2). Participants were asked to rank the 13 cards according to how well the meaning of each statement matched the given definition of ease of use or usefulness.

For the categorization task, participants were asked to put the 13 cards into three to five categories so that the statements within a category were most similar in meaning to each other and dissimilar in meaning from those in other categories. This was an adaptation of the “own categories” procedure of Sherif and Sherif (1967). Categorization provides a simple indicant of similarity that requires less time and effort to obtain than other similarity measurement procedures such as paid comparisons. The similarity data was cluster analyzed by assigning to the same cluster items that seven or more subjects placed in the same category. The clusters are considered to be a reflection of the domain substrata for each construct and serve as a basis of assessing coverage, or representativeness, of the item pools.

The resulting rank and cluster data are summarized in Tables 3 (usefulness) and 4 (ease of use). For perceived usefulness, notice that items fall into three main clusters. The first cluster relates to job effectiveness, the second to productivity and time savings, and the third to the importance of the system to one’s job. If we eliminate the lowest-ranked items (items 1, 4, 5 and 9), we see that the three major clusters each have at least two items. Item 2, “control over work” was retained since, although it was ranked fairly low, it fell in the top 9 and may tap an important aspect of usefulness.

Looking now at perceived ease of use (Table 4), we again find three main clusters. The first relates to physical effort, while the second relates to mental effort. Selecting the six highest priority items and eliminating the seventh provides good coverage of these two clusters. Item 11 (“understandable”) was reworded to read “clear and understandable” in an effort to pick up some of the content of item 1 (“confusing”), which has been eliminated. The third cluster is somewhat more difficult to interpret but appears to be tapping perceptions of how easy a system is to learn. Remembering how to perform tasks, using the manual, and relying on system guidance are all phenomena associated with the process of learning to use a new system (Nickerson, 1981; Roberts and Moran, 1983). Further review of the literature suggests that ease of use and ease of learning are strongly related. Roberts and Moran (1983) find a correlation of .79 between objective measures of ease of use and ease of learning. Whiteside, et al. (1985) find that ease of use and ease of learning are strongly related and conclude that they are congruent. Studies of how people learn new systems suggest that learning and using are not separate, disjoint activities, but instead that people are motivated to begin performing actual work directly and try to “learn by doing” as opposed to going through user manuals or online tutorials (Carroll and Carrithers, 1984; Carroll, et al., 1985; Carroll and McKendree, 1987).

In this study, therefore, ease of learning is regarded as one substratum of the ease of use construct, as opposed to a distinct construct. Since items 4 and 13 provide a rather indirect assessment of ease of learning, they were replaced with two items that more directly get at ease of learning: “Learning to operate the electronic mail system is easy for me,” and “I find it takes a lot of effort to become skillful at using electronic mail.” Items 6, 9 and 2 were eliminated because they did not cluster with other items, and they received low priority rankings, which suggests that they do not fit well within the content domain for ease of use. Together with the ‘overall’ items for each construct, this procedure yielded a 10-item scale for each construct to be empirically tested for reliability and construct validity.

## Study 1

A field study was conducted to assess the reliability, convergent validity, discriminant validity, and factorial validity of the 10-item scales resulting from the pretest. A sample of 120 users within IBM Canada’s Toronto Development Laboratory were given a questionnaire asking them to rate the usefulness and ease of use of two systems available there: PROFS electronic mail and the XEDIT file editor. The computing environment consisted of IBM mainframes accessible through 327X terminals. The PROFS electronic mail system is a simple but limited messaging facility for brief messages. (See Panko, 1988.) The XEDIT editor is widely available on IBM systems and offers both full-screen and command-driven editing capabilities. The questionnaire asked participants to rate the extent to which they agree with each statement by circling a number from one to seven arranged horizontally beneath anchor point descriptions “Strongly Agree,” “Neutral,” and “Strongly Disagree.” In order to ensure subject familiarity with the systems being rated, instructions asked the participants to skip over the section pertaining to a given system if they never use it. Responses were obtained from 112 participants, for a response rate of 93%. Of these 112, 109 were users of electronic mail and 75 were users of XEDIT. Subjects had an average of six monttis’ experience with the two systems studied. Among the sample, 10 percent were managers, 35 percent were administrative staff, and 55 percent were professional staff (which included a broad mix of market analysts, product development analysts, programmers, financial analysts and research scientists).

### Reliability and validity

The perceived usefulness scale attained Cronbach alpha reliability of .97 for both the electronic mail and XEDIT systems, while perceived ease of use achieved a reliability of .86 for electronic mail and .93 for XEDIT. When observations were pooled for the two systems, alpha was .97 for usefulness and .91 for ease of use.

Convergent and discriminant validity were tested using multitrait-multimethod (MTMM) analysis (Campbell and Fiske, 1959). The MTMM matrix contains the intercorrelations of items (methods) applied to the two different test systems (traits), electronic mail and XEDIT. Convergent validity refers to whether the items comprising a scale behave as if they are measuring a common underlying construct. In order to demonstrate convergent validity, items that measure the same trait should correlate highly with one another (Campbell and Fiske, 1959). That is, the elements in the monotrait triangles (the submatrix of intercorrelations between items intended to measure the same construct for the same system) within the MTMM matrices should be large. For perceived usefulness, the 90 monotraitheteromethod correlations were all significant at the .05 level. For ease of use, 86 out of 90, or 95.6%, of the monotrait-heteromethod correlations were significant. Thus, our data supports the convergent validity of the two scales.

Discriminant validity is concerned with the ability of a measurement item to differentiate between objects being measured. For instance, within the MTMM matrix, a perceived usefulness item applied to electronic mail should not correlate too highly with the same item applied to XEDIT. Failure to discriminate may suggest the presence of “common method variance,” which means that an item is measuring methodological artifacts unrelated to the target construct (such as individual differences in the style of responding to questions (see Campbell, et al., 1967; Silk, 1971) ). The test for discriminant validity is that an item should correlate more highly with other items intended to measure the same trait than with either the same item used to measure a different trait or with different items used to measure a different trait (Campbell and Fiske, 1959). For perceived usefulness, 1,800 such comparisons were confirmed without exception. Of the 1,800 comparisons for ease of use there were 58 exceptions (3%). This represents an unusually high level of discriminant validity (Campbell and Fiske, 1959; Silk, 1971) and implies that the usefulness and ease of use scales possess a high concentration of trait variance and are not strongly influenced by methodological artifacts.

Table 5 gives a summary frequency table of the correlations comprising the MTMM matrices for usefulness and ease of use. From this table it is possible to see the separation in magnitude between monotrait and heterotrait correlations. The frequency table also shows that the heterotrait-heteromethod correlations do not appear to be substantially elevated above the heterotraitmonomethod correlations. This is an additional diagnostic suggested by Campbell and Fiske (1959) to detect the presence of method variance.

The few exceptions to the convergent and discriminant validity that did occur, although not extensive enough to invalidate the ease of use scale, all involved negatively phrased ease of use items. These “reversed” items tended to correlate more with the same item used to measure a different trait than they did with other items of the same trait, suggesting the presence of common method variance. This is ironic, since reversed scales are typically used in an effort to reduce common method variance. Silk (1971) similarly observed minor departures from convergent and discriminant validity for reversed items. The five positively worded ease of use items had a reliability of .92 compared to .83 for the five negative items. This suggests an improvement in the ease of use scale may be possible with the elimination or reversal of negatively phrased items. Nevertheless, the MTMM analysis supported the ability of the 10-item scales for each construct to differentiate between systems. 

Factorial validity is concerned with whether the usefulness and ease of use items form distinct constructs. A principal components analysis using oblique rotation was performed on the twenty usefulness and ease of use items. Data were pooled across the two systems, for a total of 184 observations. The results show that the usefulness and ease of use items load on distinct factors (Table 6). The multitrait-multimethod analysis and factor analysis both support the construct validity of the 10-item scales.